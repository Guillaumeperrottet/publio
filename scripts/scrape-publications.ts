#!/usr/bin/env tsx
/**
 * Script de scraping des publications communales
 * Ex√©cut√© par cron job (6x/jour pour SIMAP)
 *
 * Usage:
 *   npx tsx scripts/scrape-publications.ts [--include-weekly]
 *
 * Options:
 *   --include-weekly : Inclut les sources hebdomadaires (Fribourg FO, etc.)
 */

import { prisma } from "@/lib/db/prisma";
import {
  MasterScraper,
  deduplicatePublications,
  filterRecentPublications,
} from "@/features/veille/scraper";
import { SimapScraper } from "@/features/veille/scrapers/simap";
import { FribourgOfficialGazetteScraper } from "@/features/veille/scrapers/fribourg-official";
import { ValaisOfficialScraper } from "@/features/veille/scrapers/valais-official";
import { ValaisWebScraper } from "@/features/veille/scrapers/valais-web";
import { notifyMatchingVeilleSubscriptions } from "@/features/notifications/actions";

async function scrapeAndStorePublications(includeWeekly = false) {
  console.log("=".repeat(60));
  console.log("üì° SCRAPING DES PUBLICATIONS CANTONALES");
  console.log("=".repeat(60));
  console.log(`D√©marrage: ${new Date().toISOString()}\n`);

  try {
    // 1. R√©cup√©rer tous les cantons actifs (depuis les abonnements)
    const activeSubscriptions = await prisma.veilleSubscription.findMany({
      select: { cantons: true },
    });

    const allCantons = Array.from(
      new Set(activeSubscriptions.flatMap((sub) => sub.cantons))
    );

    console.log(
      `üìç Cantons surveill√©s: ${allCantons.length} (${allCantons.join(", ")})\n`
    );

    // 2. Scraper SIMAP (plateforme nationale)
    const simapScraper = new SimapScraper();
    const simapPublications = await simapScraper.scrape(
      allCantons as (
        | "VD"
        | "GE"
        | "VS"
        | "FR"
        | "NE"
        | "JU"
        | "BE"
        | "TI"
        | "GR"
      )[]
    );

    console.log(
      `üá®üá≠ SIMAP: ${simapPublications.length} publication(s) trouv√©e(s)`
    );

    // 3. Scraper Fribourg (Feuille Officielle PDF) - uniquement si demand√©
    let fribourgPublications: any[] = [];
    if (includeWeekly && allCantons.includes("FR")) {
      console.log(`\nüìÑ Scraping Fribourg FO (hebdomadaire)...`);
      const fribourgScraper = new FribourgOfficialGazetteScraper();
      fribourgPublications = await fribourgScraper.scrape();
      console.log(
        `üìÑ Fribourg FO: ${fribourgPublications.length} publication(s) trouv√©e(s)`
      );
    } else if (!includeWeekly) {
      console.log(
        `\n‚è≠Ô∏è  Fribourg FO ignor√© (sources hebdomadaires d√©sactiv√©es)`
      );
    }

    // 4. Scraper Valais - Double approche pour couverture compl√®te
    let valaisPublications: any[] = [];
    if (allCantons.includes("VS")) {
      // 4a. PDF Bulletin Officiel (constructions, march√©s publics, annonces)
      console.log(`\nüì∞ Scraping Valais BO PDF (constructions & march√©s)...`);
      const valaisPdfScraper = new ValaisOfficialScraper();
      const valaisPdfPubs = await valaisPdfScraper.scrape();
      console.log(
        `üì∞ Valais PDF: ${valaisPdfPubs.length} publication(s) trouv√©e(s)`
      );

      // 4b. Web scraping (actes judiciaires, faillites, etc.)
      console.log(`\nüåê Scraping Valais BO Web (actes & faillites)...`);
      const valaisWebScraper = new ValaisWebScraper();
      const valaisWebPubs = await valaisWebScraper.scrape();
      console.log(
        `üåê Valais Web: ${valaisWebPubs.length} publication(s) trouv√©e(s)`
      );

      valaisPublications = [...valaisPdfPubs, ...valaisWebPubs];
    }

    // 5. Scraper les autres sources (canton-specific)
    const scraper = new MasterScraper();
    const cantonPublications = await scraper.scrapeAll();

    console.log(
      `üèõÔ∏è  Sources cantonales: ${cantonPublications.length} publication(s) trouv√©e(s)`
    );

    // 5. Combiner toutes les publications
    const rawPublications = [
      ...simapPublications,
      ...fribourgPublications,
      ...valaisPublications,
      ...cantonPublications,
    ];

    console.log(
      `\n‚úÖ Scraping termin√©: ${rawPublications.length} publication(s) brute(s)\n`
    );

    // 2. D√©duplicater et filtrer (30 derniers jours)
    let publications = deduplicatePublications(rawPublications);
    publications = filterRecentPublications(publications, 30);

    console.log(
      `\nüìä Apr√®s traitement: ${publications.length} publication(s) √† traiter\n`
    );

    if (publications.length === 0) {
      console.log("‚ÑπÔ∏è  Aucune nouvelle publication √† enregistrer");
      return { processed: 0, created: 0, skipped: 0 };
    }

    // 3. Enregistrer en base de donn√©es
    let created = 0;
    let updated = 0;
    let skipped = 0;

    for (const pub of publications) {
      try {
        // Pour SIMAP, utiliser projectNumber comme identifiant unique
        // Pour les autres sources, utiliser l'URL
        const isSIMAP = pub.metadata?.source === "SIMAP";
        const projectNumber = pub.metadata?.projectNumber;

        let existing = null;

        if (isSIMAP && projectNumber) {
          // Chercher par projectNumber SIMAP
          existing = await prisma.veillePublication.findFirst({
            where: {
              metadata: {
                path: ["projectNumber"],
                equals: projectNumber,
              },
            },
          });
        } else {
          // Chercher par URL pour les autres sources
          existing = await prisma.veillePublication.findFirst({
            where: {
              url: pub.url,
              commune: pub.commune,
            },
          });
        }

        if (existing) {
          // Mettre √† jour l'URL si elle a chang√© (important pour SIMAP)
          if (existing.url !== pub.url) {
            await prisma.veillePublication.update({
              where: { id: existing.id },
              data: {
                url: pub.url,
                title: pub.title,
                description: pub.description || null,
                metadata: pub.metadata || {},
              },
            });
            updated++;
            console.log(
              `üîÑ Mise √† jour: ${pub.commune} - ${pub.title.substring(
                0,
                50
              )}...`
            );
          } else {
            skipped++;
            console.log(
              `‚è≠Ô∏è  Inchang√©e: ${pub.commune} - ${pub.title.substring(0, 50)}...`
            );
          }
          continue;
        }

        // Cr√©er la nouvelle publication
        const newPublication = await prisma.veillePublication.create({
          data: {
            title: pub.title,
            description: pub.description || null,
            url: pub.url,
            commune: pub.commune,
            canton: pub.canton,
            type: pub.type,
            publishedAt: pub.publishedAt,
            metadata: pub.metadata || {},
          },
        });

        created++;
        console.log(
          `‚úÖ Cr√©√©e: ${pub.commune} - ${pub.title.substring(0, 50)}...`
        );

        // Notify matching veille subscriptions
        try {
          await notifyMatchingVeilleSubscriptions(newPublication.id);
        } catch (notifError) {
          console.error(
            `‚ö†Ô∏è  Error notifying subscriptions for ${newPublication.id}:`,
            notifError
          );
          // Don't block the scraping if notification fails
        }
      } catch (error) {
        console.error(
          `‚ùå Erreur lors de l'enregistrement de "${pub.title}":`,
          error
        );
      }
    }

    console.log("\n" + "=".repeat(60));
    console.log("üìä R√âSUM√â");
    console.log("=".repeat(60));
    console.log(`Publications scrap√©es:    ${rawPublications.length}`);
    console.log(`Apr√®s d√©duplications:     ${publications.length}`);
    console.log(`Nouvelles cr√©√©es:         ${created}`);
    console.log(`URLs mises √† jour:        ${updated}`);
    console.log(`D√©j√† existantes:          ${skipped}`);
    console.log("=".repeat(60));
    console.log(`Termin√©: ${new Date().toISOString()}`);

    return { processed: publications.length, created, updated, skipped };
  } catch (error) {
    console.error("\n‚ùå ERREUR CRITIQUE:", error);
    throw error;
  }
}

// Ex√©cuter le script
const includeWeekly = process.argv.includes("--include-weekly");

scrapeAndStorePublications(includeWeekly)
  .then(() => {
    console.log("\n‚úÖ Script termin√© avec succ√®s");
    process.exit(0);
  })
  .catch((error) => {
    console.error("\n‚ùå Le script a √©chou√©:", error);
    process.exit(1);
  });
